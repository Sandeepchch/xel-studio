#!/usr/bin/env python3
"""
News Pipeline Worker ‚Äî GitHub Actions Background Runner
========================================================
Pipeline: Dynamic Query ‚Üí Tavily Search ‚Üí URL Dedup ‚Üí
          Cerebras (GPT-OSS 120B / llama3.1-8b) ‚Üí FLUX.1-dev Image Gen ‚Üí
          Cloudinary Upload ‚Üí Firestore Save ‚Üí History Update

Ported from: app/api/cron/generate-news/route.ts (v17)
"""

import json
import os
import random
import sys
import time
import uuid
from datetime import datetime, timezone
from urllib.parse import urlparse, urlencode, parse_qs, urlunparse

import cloudinary
import cloudinary.uploader
import firebase_admin
from firebase_admin import credentials, firestore
import requests
from cerebras.cloud.sdk import Cerebras

# ‚îÄ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

COLLECTION = "news"
HISTORY_COLLECTION = "news_history"
HEALTH_DOC_PATH = "system/cron_health"
HISTORY_TTL_DAYS = 10
TAVILY_RESULT_COUNT = 10

IMAGE_WIDTH = 1024
IMAGE_HEIGHT = 576  # 16:9 cinematic ratio

# ‚îÄ‚îÄ‚îÄ Search Queries (Balanced: ~50% AI/Tech, ~50% Diverse) ‚îÄ‚îÄ‚îÄ
#
# Distribution:
#   ~50% ‚Üí AI, Tech, Hardware, Robotics (primary focus)
#   ~50% ‚Üí Disability/Accessibility, Climate/Environment, World Affairs,
#           CEO/Business Leaders, Science, Health, Culture
#
# Each pipeline run picks ONE random query, so over time the mix balances out.

QUERY_BUCKETS = {
    # ‚îÄ‚îÄ AI & Tech (core) ‚îÄ‚îÄ
    "ai-tech": [
        "artificial intelligence latest breakthroughs announcements",
        "OpenAI GPT new model release announcements",
        "Google DeepMind Gemini AI research news",
        "Anthropic Claude AI safety research news",
        "Meta AI Llama open source model news",
        "generative AI tools products launches today",
        "AI startup funding acquisition deals news",
        "AI regulation policy government updates",
        "Nvidia AMD AI chip semiconductor hardware news",
        "quantum computing breakthrough research news",
        "robotics automation humanoid robot news",
        "AI coding programming developer tools news",
        "AI image video generation model news",
        "cloud computing AI infrastructure updates",
    ],

    # ‚îÄ‚îÄ Open Source AI ‚îÄ‚îÄ
    "open-source": [
        "open source AI models community development news",
        "Hugging Face open source AI tools models news",
        "Mistral AI open source language model news",
        "open source large language model release news",
        "Linux open source software community news",
        "open source AI framework PyTorch TensorFlow news",
    ],

    # ‚îÄ‚îÄ Disability & Accessibility ‚îÄ‚îÄ
    "disability": [
        "disability technology assistive tech accessibility news",
        "AI assistive technology disability inclusion news",
        "accessible technology innovations disabled people news",
        "visually impaired blind students assistive technology news",
        "screen reader accessibility blind people technology news",
        "deaf hearing impaired technology accessibility news",
        "wheelchair disability mobility technology innovation news",
        "autism neurodiversity technology support news",
    ],

    # ‚îÄ‚îÄ Health ‚îÄ‚îÄ
    "health": [
        "healthcare technology innovation AI medical news",
        "mental health digital wellness technology news",
        "AI healthcare diagnosis treatment breakthrough news",
        "medical technology health research discovery news",
        "telemedicine digital health innovation news",
        "drug discovery AI pharmaceutical research news",
    ],

    # ‚îÄ‚îÄ Climate & Natural Disasters ‚îÄ‚îÄ
    "climate": [
        "climate change global warming research news today",
        "climate technology clean energy innovation news",
        "earthquake volcano natural disaster news today",
        "extreme weather flooding hurricane disaster news",
        "renewable energy solar wind power news",
        "climate policy carbon emissions sustainability news",
        "wildlife conservation biodiversity environmental news",
    ],

    # ‚îÄ‚îÄ World Affairs ‚îÄ‚îÄ
    "world": [
        "geopolitical technology competition world news",
        "international trade technology policy news",
        "digital privacy surveillance regulation world news",
        "global economy recession inflation news today",
        "war conflict peace diplomatic negotiations news",
        "election democracy political news today",
        "refugee migration humanitarian crisis news",
    ],

    # ‚îÄ‚îÄ General / Business / Science ‚îÄ‚îÄ
    "general": [
        "tech CEO statements leadership announcements news",
        "tech company earnings big tech stock news",
        "Apple Google Microsoft major tech announcements",
        "cryptocurrency blockchain Web3 news",
        "social media platform changes updates news",
        "science discovery research breakthrough news",
        "space technology SpaceX NASA launch news",
        "gaming esports streaming industry news",
    ],
}

# Rotation order ‚Äî ensures each category gets coverage across the day
# 48 runs/day (every 30 min) spread across 7 categories
ROTATION_ORDER = [
    "ai-tech", "disability", "climate", "open-source",
    "health", "world", "general", "ai-tech",
    "open-source", "disability", "ai-tech", "climate",
    "world", "health", "general", "ai-tech",
    "disability", "open-source", "climate", "health",
    "ai-tech", "world", "general", "disability",
    "open-source", "ai-tech", "climate", "health",
    "world", "general", "ai-tech", "disability",
    "climate", "open-source", "health", "ai-tech",
    "world", "general", "disability", "climate",
    "open-source", "health", "ai-tech", "world",
    "general", "disability", "ai-tech", "climate",
]


def pick_search_query() -> tuple[str, str]:
    """Pick a search query based on time-of-day rotation.
    Returns (query, category_key)."""
    from datetime import datetime, timezone
    now = datetime.now(timezone.utc)
    # Slot index: each 30-min slot gets a category
    slot = (now.hour * 2 + (1 if now.minute >= 30 else 0)) % len(ROTATION_ORDER)
    category_key = ROTATION_ORDER[slot]
    queries = QUERY_BUCKETS[category_key]
    query = random.choice(queries)
    return query, category_key


def pick_fallback_queries(exclude_category: str) -> list[tuple[str, str]]:
    """Pick queries from OTHER categories for fallback."""
    fallbacks = []
    other_keys = [k for k in QUERY_BUCKETS if k != exclude_category]
    random.shuffle(other_keys)
    for key in other_keys[:3]:  # Try 3 different categories
        q = random.choice(QUERY_BUCKETS[key])
        fallbacks.append((q, key))
    return fallbacks

# ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def detect_category(query: str, title: str = "", content: str = "") -> str:
    """Detect category from search query, title, and article content.
    Checks ALL text for keyword matches, with priority weighting."""
    # Combine all text for analysis (title gets extra weight by appearing twice)
    q = f"{query} {title} {title} {content[:500]}".lower()

    # Category rules: ordered from most specific to least
    CATEGORY_RULES = [
        ("disability", [
            "disability", "disabled", "assistive", "accessible", "accessibility",
            "inclusion", "wheelchair", "blind", "deaf", "autism", "neurodiversity",
            "ada ", "special needs", "impairment", "prosthetic", "screen reader",
        ]),
        ("health", [
            "healthcare", "health", "mental health", "wellness", "medical",
            "disease", "vaccine", "hospital", "patient", "therapy", "drug ",
            "pharmaceutical", "clinical trial", "who ", "cdc ",
        ]),
        ("climate", [
            "climate", "environment", "clean energy", "sustainability",
            "energy transition", "carbon", "emissions", "renewable", "solar",
            "wind energy", "ev ", "electric vehicle", "green",
        ]),
        ("science", [
            "space", "spacex", "nasa", "physics", "astronomy", "mars",
            "biotechnology", "genetics", "science discovery", "research breakthrough",
            "quantum", "crispr", "genome", "telescope", "satellite",
        ]),
        ("world", [
            "geopolitical", "international", "trade war", "privacy", "surveillance",
            "world", "regulation", "government", "policy", "law ", "legislation",
            "congress", "parliament", "sanctions", "diplomacy", "united nations",
            "eu ", "european union", "china", "india", "nist", "ftc",
        ]),
        ("business", [
            "earnings", "stock", "ipo", "funding", "startup", "unicorn",
            "crypto", "blockchain", "web3", "ceo", "revenue", "acquisition",
            "merger", "market cap", "investor", "venture capital", "valuation",
        ]),
        ("entertainment", [
            "social media", "streaming", "gaming", "esports", "entertainment",
            "movie", "music", "tiktok", "youtube", "netflix", "spotify",
        ]),
    ]

    # Score each category by keyword matches
    scores: dict[str, int] = {}
    for cat, keywords in CATEGORY_RULES:
        score = sum(1 for kw in keywords if kw in q)
        if score > 0:
            scores[cat] = score

    if scores:
        best_cat = max(scores, key=scores.get)
        if scores[best_cat] >= 1:
            return best_cat

    # AI & Tech (default for anything AI/tech related)
    return "ai-tech"


def extract_topic(query: str) -> str:
    """Remove time modifiers to get the core topic."""
    import re
    time_pattern = r"\s*(latest breaking news|updates today|news \w+ \d+|fresh developments|this week|breaking today|\d+ breakthrough|exclusive update)$"
    topic = re.sub(time_pattern, "", query, flags=re.IGNORECASE).strip()
    topic = re.sub(r"\s+(AND|OR)\s+", " & ", topic)
    return topic


def normalize_url(url: str) -> str:
    """Normalize URL for consistent comparison."""
    try:
        parsed = urlparse(url)
        hostname = parsed.hostname.lower() if parsed.hostname else ""
        pathname = parsed.path.rstrip("/")
        # Remove tracking params
        params = parse_qs(parsed.query)
        for key in ["utm_source", "utm_medium", "utm_campaign", "utm_content", "utm_term", "ref", "source"]:
            params.pop(key, None)
        clean_query = urlencode(params, doseq=True) if params else ""
        return urlunparse(("https", hostname, pathname, "", clean_query, ""))
    except Exception:
        return url.lower().rstrip("/")


# ‚îÄ‚îÄ‚îÄ Firebase Init ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def init_firebase() -> firestore.Client:
    """Initialize Firebase Admin SDK from environment variables."""
    if firebase_admin._apps:
        return firestore.client()

    # Check both env var names (GitHub Actions uses FIREBASE_CREDENTIALS,
    # local .env.local uses FIREBASE_SERVICE_ACCOUNT)
    creds_json = os.environ.get("FIREBASE_CREDENTIALS") or os.environ.get("FIREBASE_SERVICE_ACCOUNT")
    if creds_json:
        try:
            creds_dict = json.loads(creds_json)
            cred = credentials.Certificate(creds_dict)
            firebase_admin.initialize_app(cred)
            print("üî• Firebase initialized from FIREBASE_CREDENTIALS")
            return firestore.client()
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to parse FIREBASE_CREDENTIALS: {e}")

    # Fallback: individual env vars
    project_id = os.environ.get("FIREBASE_PROJECT_ID", "xelbackend")
    client_email = os.environ.get("FIREBASE_CLIENT_EMAIL")
    private_key = os.environ.get("FIREBASE_PRIVATE_KEY", "").replace("\\n", "\n")

    if client_email and private_key:
        cred = credentials.Certificate({
            "type": "service_account",
            "project_id": project_id,
            "client_email": client_email,
            "private_key": private_key,
            "token_uri": "https://oauth2.googleapis.com/token",
        })
        firebase_admin.initialize_app(cred)
        print("üî• Firebase initialized from individual env vars")
        return firestore.client()

    firebase_admin.initialize_app(options={"projectId": project_id})
    print("üî• Firebase initialized with default credentials")
    return firestore.client()


# ‚îÄ‚îÄ‚îÄ Cloudinary Init ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def init_cloudinary():
    """Initialize Cloudinary from CLOUDINARY_URL env var."""
    url = os.environ.get("CLOUDINARY_URL")
    if url:
        cloudinary.config(cloudinary_url=url)
        print("‚òÅÔ∏è Cloudinary initialized")
    else:
        cloud_name = os.environ.get("CLOUDINARY_CLOUD_NAME")
        api_key = os.environ.get("CLOUDINARY_API_KEY")
        api_secret = os.environ.get("CLOUDINARY_API_SECRET")
        if cloud_name and api_key and api_secret:
            cloudinary.config(
                cloud_name=cloud_name,
                api_key=api_key,
                api_secret=api_secret,
            )
            print("‚òÅÔ∏è Cloudinary initialized from individual env vars")
        else:
            print("‚ö†Ô∏è No Cloudinary credentials ‚Äî images will use placeholder")





# ‚îÄ‚îÄ‚îÄ Tavily Search ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def search_tavily(query: str, days_back: int = 3) -> dict:
    """Search Tavily with dual-key fallback. Returns {context, results}."""
    keys = [
        os.environ.get("TAVILY_API_KEY"),
        os.environ.get("TAVILY_API_KEY_2"),
    ]
    keys = [k for k in keys if k]

    if not keys:
        print("‚ö†Ô∏è No TAVILY_API_KEY set ‚Äî skipping search")
        return {"context": "", "results": []}

    for i, key in enumerate(keys):
        label = "primary" if i == 0 else "fallback"
        try:
            print(f'üîç Tavily ({label}): searching "{query}" (last {days_back} days)...')
            resp = requests.post(
                "https://api.tavily.com/search",
                json={
                    "api_key": key,
                    "query": query,
                    "search_depth": "advanced",
                    "topic": "news",
                    "days": days_back,
                    "max_results": TAVILY_RESULT_COUNT,
                    "include_answer": False,
                },
                timeout=30,
            )
            resp.raise_for_status()
            data = resp.json()

            results = data.get("results", [])
            if not results:
                print(f'‚ö†Ô∏è Tavily ({label}) returned no results for "{query}"')
                continue

            mapped = [
                {"title": r.get("title", ""), "description": r.get("content", ""), "url": r.get("url", "")}
                for r in results
            ]
            context = "\n\n".join(
                f"[{j+1}] {r['title']}\n{r['description']}" for j, r in enumerate(mapped)
            )
            print(f'üîç Tavily ({label}): {len(mapped)} results for "{query}"')
            return {"context": context, "results": mapped}

        except Exception as e:
            print(f"‚ö†Ô∏è Tavily ({label}) failed: {e}")
            if i < len(keys) - 1:
                print("üîÑ Switching to fallback Tavily API key...")

    print("‚ö†Ô∏è All Tavily keys exhausted ‚Äî no results")
    return {"context": "", "results": []}


# ‚îÄ‚îÄ‚îÄ URL History & LLM Dedup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def load_history_urls(db: firestore.Client) -> set[str]:
    """Load all source URLs from news_history into a Set for O(1) lookup."""
    docs = db.collection(HISTORY_COLLECTION).select(["sourceUrls"]).stream()
    urls = set()
    doc_count = 0
    for doc in docs:
        doc_count += 1
        data = doc.to_dict()
        source_urls = data.get("sourceUrls", [])
        if isinstance(source_urls, list):
            for u in source_urls:
                urls.add(normalize_url(u))
    print(f"üìö History loaded: {len(urls)} known URLs from {doc_count} articles")
    return urls


def load_existing_titles(db: firestore.Client) -> list[str]:
    """Load ALL existing titles from both 'news' and 'news_history'.
    The LLM uses this list to avoid generating repeated/similar topics."""
    titles = []
    # From live news collection
    try:
        for doc in db.collection(COLLECTION).select(["title"]).stream():
            t = doc.to_dict().get("title", "")
            if t:
                titles.append(t)
    except Exception:
        pass
    # From history (deleted articles ‚Äî still need dedup)
    try:
        for doc in db.collection(HISTORY_COLLECTION).select(["title"]).stream():
            t = doc.to_dict().get("title", "")
            if t:
                titles.append(t)
    except Exception:
        pass
    print(f"üìã Dedup: loaded {len(titles)} existing titles (news + history)")
    return titles


def filter_by_url_history(results: list[dict], known_urls: set[str]) -> tuple[list[dict], int]:
    """Filter Tavily results ‚Äî remove any whose URL matches history."""
    fresh = [r for r in results if normalize_url(r.get("url", "")) not in known_urls]
    filtered = len(results) - len(fresh)
    if filtered > 0:
        print(f"üîó URL filter: {filtered} already-used URLs removed, {len(fresh)} fresh results remain")
    return fresh, filtered


def save_to_history(db: firestore.Client, title: str, content: str, source_urls: list[str]):
    """Save generated article metadata + source URLs to history."""
    try:
        normalized = [normalize_url(u) for u in source_urls]
        db.collection(HISTORY_COLLECTION).add({
            "title": title,
            "content": content,
            "sourceUrls": normalized,
            "createdAt": datetime.now(timezone.utc).isoformat(),
        })
        print(f'üìö History saved: "{title[:50]}" with {len(normalized)} URLs')
    except Exception as e:
        print(f"‚ö†Ô∏è History save failed (non-critical): {e}")


# ‚îÄ‚îÄ‚îÄ Health Tracking ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def log_health(db: firestore.Client, status: str, details: dict):
    """Update system/cron_health document."""
    try:
        now = datetime.now(timezone.utc)
        db.document(HEALTH_DOC_PATH).set({
            "status": status,
            "timestamp": now.isoformat(),
            "last_run": now.strftime("%d/%m/%Y, %I:%M:%S %p"),
            "runner": "github-actions",
            **details,
        })
    except Exception as e:
        print(f"Health log write failed: {e}")


# ‚îÄ‚îÄ‚îÄ Image Generation (g4f multi-provider) & Cloudinary Upload ‚îÄ‚îÄ‚îÄ

# Priority 1: g4f (Flux, DALL-E 3, SDXL, SD3 ‚Äî no API keys needed)
# Priority 2: Placeholder image

PLACEHOLDER_IMAGE_URL = (
    "https://placehold.co/1024x576/1a1a2e/e2e8f0?text=XeL+AI+News&font=roboto"
)


def _upload_placeholder_to_cloudinary(article_id: str) -> str:
    """Upload a placeholder image to Cloudinary, or return static URL as ultimate fallback."""
    print(f"  üîÑ Uploading placeholder to Cloudinary...")
    try:
        placeholder_bytes = requests.get(PLACEHOLDER_IMAGE_URL, timeout=15).content
        if placeholder_bytes and len(placeholder_bytes) > 500:
            result = cloudinary.uploader.upload(
                placeholder_bytes,
                public_id=article_id,
                folder="xel-news",
                resource_type="image",
                overwrite=True,
            )
            placeholder_url = result.get("secure_url", "")
            if placeholder_url:
                print(f"  ‚úÖ Placeholder uploaded: {placeholder_url[:80]}...")
                return placeholder_url
    except Exception as e:
        print(f"  ‚ö†Ô∏è Placeholder upload failed: {e}")

    print(f"  ‚ö†Ô∏è Using static placeholder URL")
    return PLACEHOLDER_IMAGE_URL





def _upload_bytes_to_cloudinary(image_bytes: bytes, article_id: str) -> str | None:
    """Upload raw image bytes to Cloudinary, return secure URL or None."""
    try:
        print(f"  ‚òÅÔ∏è Uploading to Cloudinary (public_id=xel-news/{article_id})...")
        result = cloudinary.uploader.upload(
            image_bytes,
            public_id=article_id,
            folder="xel-news",
            resource_type="image",
            overwrite=True,
        )
        url = result.get("secure_url", "")
        if url:
            print(f"  ‚òÅÔ∏è Cloudinary URL: {url[:80]}...")
            print(f"  ‚òÅÔ∏è Format: {result.get('format')}, "
                  f"Size: {result.get('bytes')} bytes, "
                  f"Dims: {result.get('width')}x{result.get('height')}")
            return url
    except Exception as e:
        print(f"  ‚ùå Cloudinary upload failed: {e}")
    return None


def _call_g4f_image(prompt: str) -> bytes | None:
    """Attempt image generation via g4f multi-provider system."""
    try:
        from gemini_image_gen import generate_image_gemini
        return generate_image_gemini(prompt)
    except ImportError:
        print("  ‚ö†Ô∏è g4f image gen not available (g4f not installed?)")
        return None
    except Exception as e:
        print(f"  ‚ùå g4f image error: {e}")
        return None


def generate_and_upload_image(prompt: str, article_id: str) -> str:
    """
    Image pipeline:
      1. g4f (Flux, DALL-E 3, SDXL, SD3) ‚Üí Cloudinary
      2. Placeholder ‚Üí Cloudinary
    """
    import re as _re

    print(f"\n{'‚îÄ'*50}")
    print("üñºÔ∏è IMAGE PIPELINE (g4f ‚Üí Placeholder)")
    print(f"   Article ID: {article_id}")
    print(f"{'‚îÄ'*50}")

    # Sanitize prompt
    clean_prompt = _re.sub(r"[^\w\s,.\-!?']", "", prompt)
    clean_prompt = _re.sub(r"\s+", " ", clean_prompt).strip()
    if len(clean_prompt) > 300:
        clean_prompt = clean_prompt[:300].rsplit(" ", 1)[0]

    enhanced_prompt = clean_prompt
    print(f"   Prompt: \"{clean_prompt[:80]}...\"")

    # ‚îÄ‚îÄ Attempt 1: g4f (multi-provider) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    g4f_bytes = _call_g4f_image(enhanced_prompt)
    if g4f_bytes:
        result = _upload_bytes_to_cloudinary(g4f_bytes, article_id)
        if result:
            print(f"  ‚úÖ IMAGE SUCCESS (g4f ‚Üí Cloudinary)")
            return result

    # ‚îÄ‚îÄ Attempt 2: Placeholder ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    print(f"  ‚ö†Ô∏è g4f failed, using placeholder")
    return _upload_placeholder_to_cloudinary(article_id)


# ‚îÄ‚îÄ‚îÄ Parse JSON Response ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def parse_article_response(text: str) -> tuple[str, str]:
    """Extract articleText and category from JSON response.
    Returns (article_text, category)."""
    clean = text.strip()
    if clean.startswith("```"):
        import re
        clean = re.sub(r"^```(?:json)?\s*\n?", "", clean)
        clean = re.sub(r"\n?```\s*$", "", clean)
    try:
        parsed = json.loads(clean)
        article = parsed.get("articleText", "").strip() if "articleText" in parsed else clean
        category = parsed.get("category", "").strip().lower() if "category" in parsed else ""
        # Validate category is one of the allowed values
        valid_categories = {"ai-tech", "disability", "health", "world", "general"}
        if category not in valid_categories:
            category = ""
        return (article, category)
    except json.JSONDecodeError:
        pass
    return (clean, "")


# ‚îÄ‚îÄ‚îÄ Cerebras LLM ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def call_cerebras(client: Cerebras, model: str, system_prompt: str, user_prompt: str) -> tuple[str, str]:
    """Call Cerebras API and return (article_text, category)."""
    completion = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.4,
        max_tokens=4096,
        response_format={"type": "json_object"},
    )
    raw = (completion.choices[0].message.content or "").strip()
    if not raw:
        raise ValueError("Empty response from Cerebras")
    return parse_article_response(raw)


# ‚îÄ‚îÄ‚îÄ Cleanup Old News ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def cleanup_old_news(db: firestore.Client):
    """
    Cleanup: keep the newest 30 articles, delete any excess.
      - Only deletes articles beyond the 30-article threshold
      - Archives deleted article URLs to 'news_history' for dedup
      - Purges 'news_history' entries older than 10 days
    """
    print("\nüßπ CLEANUP ‚Äî Checking news collection...")

    MIN_ARTICLES_TO_KEEP = 50  # Always keep at least this many articles

    # ‚îÄ‚îÄ 1. Delete excess articles beyond MIN_ARTICLES_TO_KEEP ‚îÄ‚îÄ
    try:
        # Get all news ordered by date ASCENDING (oldest first)
        all_news = list(
            db.collection(COLLECTION)
            .order_by("date", direction=firestore.Query.ASCENDING)
            .stream()
        )
        total = len(all_news)

        if total <= MIN_ARTICLES_TO_KEEP:
            print(f"  ‚úÖ {total} articles (under {MIN_ARTICLES_TO_KEEP} limit) ‚Äî no cleanup needed")
        else:
            # Only delete the EXCESS beyond the threshold
            excess = total - MIN_ARTICLES_TO_KEEP
            to_delete = all_news[:excess]
            batch = db.batch()
            count = 0

            for doc_snap in to_delete:
                data = doc_snap.to_dict()

                # ‚îÄ‚îÄ Archive to news_history BEFORE deleting ‚îÄ‚îÄ
                source_urls = data.get("sourceUrls", [])
                title = data.get("title", "")
                if source_urls or title:
                    try:
                        db.collection(HISTORY_COLLECTION).add({
                            "title": title,
                            "sourceUrls": source_urls,
                            "createdAt": datetime.now(timezone.utc).isoformat(),
                            "archivedFrom": "cleanup",
                        })
                    except Exception as he:
                        print(f"  ‚ö†Ô∏è History archive failed for '{title[:40]}': {he}")

                batch.delete(doc_snap.reference)
                count += 1
                # Firestore batch limit is 500
                if count % 400 == 0:
                    batch.commit()
                    batch = db.batch()
            if count % 400 != 0:
                batch.commit()
            print(f"  üóëÔ∏è Deleted {count} excess articles (had {total}, keeping {MIN_ARTICLES_TO_KEEP})")


    except Exception as e:
        print(f"  ‚ö†Ô∏è News cleanup failed: {e}")

    # ‚îÄ‚îÄ 2. Purge history older than 10 days ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # After 10 days, dedup entries are no longer needed
    try:
        cutoff = datetime.now(timezone.utc).isoformat()
        # Calculate cutoff: 10 days ago
        from datetime import timedelta
        cutoff_dt = datetime.now(timezone.utc) - timedelta(days=HISTORY_TTL_DAYS)
        cutoff_iso = cutoff_dt.isoformat()

        old_history = list(
            db.collection(HISTORY_COLLECTION)
            .where("createdAt", "<", cutoff_iso)
            .stream()
        )
        if old_history:
            batch = db.batch()
            count = 0
            for doc_snap in old_history:
                batch.delete(doc_snap.reference)
                count += 1
                if count % 400 == 0:
                    batch.commit()
                    batch = db.batch()
            if count % 400 != 0:
                batch.commit()
            print(f"  üóëÔ∏è Purged {count} history entries older than {HISTORY_TTL_DAYS} days")
        else:
            print(f"  ‚úÖ No history entries older than {HISTORY_TTL_DAYS} days")
    except Exception as e:
        print(f"  ‚ö†Ô∏è History cleanup failed: {e}")

    print("üßπ Cleanup complete\n")


# ‚îÄ‚îÄ‚îÄ Main Pipeline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def generate_news():
    t0 = time.time()
    print("‚ö° NEWS PIPELINE (GitHub Actions) ‚Äî Cerebras + Tavily + g4f + Cloudinary")

    # Init services
    db = init_firebase()
    init_cloudinary()

    # NOTE: Cleanup is now a separate daily cron job (news_cleanup.yml)
    # Runs once at 12:15 AM IST ‚Äî keeps 50 articles, deletes excess

    cerebras_key = os.environ.get("CEREBRAS_API_KEY")
    if not cerebras_key:
        raise RuntimeError("CEREBRAS_API_KEY not set")
    cerebras_client = Cerebras(api_key=cerebras_key)

    # 1. Pick search query via time-based rotation
    search_query, query_category = pick_search_query()
    print(f"üì∞ Query [{query_category}]: {search_query}")

    # 2. Detect category from query
    category = detect_category(search_query)
    topic = extract_topic(search_query)
    print(f"üìå Category: {category.upper()}, Topic: \"{topic}\"")

    # 3. Load URL history + existing titles for LLM dedup + Run Tavily search
    known_urls = load_history_urls(db)
    existing_titles = load_existing_titles(db)
    initial_result = search_tavily(search_query, 7)

    # 4. Filter by URL history
    scraped_data = initial_result["results"]
    used_query = search_query
    total_filtered = 0

    fresh_results, filtered_count = filter_by_url_history(scraped_data, known_urls)
    scraped_data = fresh_results
    total_filtered = filtered_count

    total_text = sum(len(f"{r.get('title','')} {r.get('description','')}") for r in scraped_data)

    if not scraped_data or total_text < 50:
        # Fallback: try queries from OTHER categories (up to 3)
        fallback_queries = pick_fallback_queries(query_category)
        found_fallback = False
        for fb_query, fb_cat in fallback_queries:
            print(f"‚ö†Ô∏è Primary search weak. Trying [{fb_cat}]: \"{fb_query}\"")
            fb_result = search_tavily(fb_query, 7)
            fb_fresh, fb_filtered = filter_by_url_history(fb_result["results"], known_urls)
            if fb_fresh and sum(len(f"{r.get('title','')} {r.get('description','')}") for r in fb_fresh) >= 50:
                scraped_data = fb_fresh
                total_filtered += fb_filtered
                used_query = fb_query
                category = detect_category(fb_query)
                print(f"‚úÖ Fallback [{fb_cat}] succeeded: {len(scraped_data)} fresh results")
                found_fallback = True
                break
            else:
                print(f"  ‚ö†Ô∏è [{fb_cat}] also empty, trying next...")

        if not found_fallback:
            # Last resort: very broad search
            print("‚ö†Ô∏è All category fallbacks empty. Trying ultra-broad search...")
            broad_result = search_tavily("latest breaking news today", 7)
            broad_fresh, br_filtered = filter_by_url_history(broad_result["results"], known_urls)
            if broad_fresh:
                scraped_data = broad_fresh
                total_filtered += br_filtered
                used_query = "latest breaking news today"
                print(f"‚úÖ Broad search succeeded: {len(scraped_data)} fresh results")
            else:
                raise RuntimeError("No fresh search results found after all fallbacks")
    else:
        print(f"‚úÖ Primary search OK: {len(scraped_data)} fresh results, {total_text} chars ({filtered_count} filtered)")

    source_urls = [r.get("url", "") for r in scraped_data if r.get("url")]

    # 5. Cerebras article generation (with LLM dedup)
    system_prompt = (
        'You are a factual tech journalist. Output valid JSON: {"articleText": "...", "category": "..."}. '
        'No other keys, no markdown, no explanation.'
    )

    cerebras_data = [{"title": r["title"], "description": r["description"]} for r in scraped_data]

    # Build dedup context ‚Äî show LLM what already exists so it doesn't repeat
    dedup_section = ""
    if existing_titles:
        # Show last 30 titles max to save tokens
        recent_titles = existing_titles[-30:]
        titles_list = "\n".join(f"- {t}" for t in recent_titles)
        dedup_section = f"""\n\nALREADY PUBLISHED (DO NOT REPEAT these topics):\n{titles_list}\n\nYou MUST pick a DIFFERENT story from the search results. If all results overlap with published titles, find a unique angle."""

    user_prompt = f"""Write a news summary from the search results below.{dedup_section}

Search results:
{json.dumps(cerebras_data, indent=2)}

STRICT FORMATTING RULES:
1. Word Count: strictly between 100 to 150 words.
2. Structure: Do NOT write paragraphs. Use exactly 3 to 4 bullet points.
3. Bold Starting Keywords (CRITICAL): Each bullet point MUST start with a **Bolded Subject, Entity, or Keyword** (e.g., **Gold**, **Microsoft**, **The global market**), followed immediately by the rest of the sentence in regular text.
4. Tone: Factual, objective, punchy. No fluff, no adjectives, no dramatic words.
5. No Title: Do NOT generate any title or heading. Output ONLY the bullet points.
6. Pick ONE story from the results. Do NOT mix topics.
7. No dates, no "breaking news" labels, no system details.
8. Use SIMPLE, CLEAR language anyone can understand.
9. YOU MUST decide the category. Pick ONE from: ai-tech, disability, health, world, general
   - ai-tech: AI, technology, open source AI, startups, chips, coding, Anthropic, OpenAI, etc.
   - disability: assistive tech, blind, deaf, wheelchair, accessibility, visually impaired, inclusion
   - health: healthcare, medical, mental health, wellness, disease, treatment
   - world: geopolitics, regulation, policy, climate, environment, international trade
   - general: business, earnings, crypto, entertainment, gaming, social media, anything else

Return JSON: {{ "articleText": "your bullet points", "category": "one-of-the-five" }}"""

    MODELS = ["gpt-oss-120b", "llama3.1-8b"]
    article_text = ""
    used_model = ""

    for model_name in MODELS:
        try:
            print(f"üîÑ Trying Cerebras model: {model_name}")
            article_text, ai_category = call_cerebras(cerebras_client, model_name, system_prompt, user_prompt)
            used_model = model_name

            if ai_category:
                print(f"ü§ñ AI picked category: {ai_category}")

            word_count = len(article_text.split())
            print(f"üìù First attempt: {word_count} words")

            # Auto-retry if too short
            if word_count < 80:
                print(f"‚ö†Ô∏è Too short ({word_count} words), retrying...")
                retry_prompt = f"""{user_prompt}

CRITICAL CORRECTION: Your previous attempt was ONLY {word_count} words. UNACCEPTABLE.
You MUST write between 100 to 150 words using 3-4 bullet points.
Each bullet MUST start with **Bold Keyword**. ADD more factual details."""

                try:
                    retry_text, retry_cat = call_cerebras(cerebras_client, model_name, system_prompt, retry_prompt)
                    retry_wc = len(retry_text.split())
                    print(f"üìù Retry: {retry_wc} words")
                    if retry_wc > word_count:
                        article_text = retry_text
                        if retry_cat:
                            ai_category = retry_cat
                        print(f"‚úÖ Retry accepted: {retry_wc} words")
                except Exception:
                    print("‚ö†Ô∏è Retry failed, keeping first attempt")

            print(f"‚úÖ Success with: {model_name}")
            break
        except Exception as e:
            print(f"‚ö†Ô∏è {model_name} failed: {str(e)[:200]}")
            article_text = ""

    if not article_text:
        raise RuntimeError("All Cerebras models failed for article generation")

    word_count = len(article_text.split())
    print(f"üìù Article ({used_model}): {word_count} words")

    # 6. Generate professional headline via LLM (MUST come before image prompt)
    title = ""
    try:
        title_completion = cerebras_client.chat.completions.create(
            model="llama3.1-8b",
            messages=[
                {
                    "role": "system",
                    "content": "Write one professional news headline. Output ONLY the headline. No quotes, no labels, no colons.",
                },
                {
                    "role": "user",
                    "content": (
                        f"Write ONE headline for this article. 8-14 words, Title Case. "
                        f"Start with WHO/WHAT. Use active verb. "
                        f"NO prefixes like 'Breaking:', 'AI News:', 'Tech:'. NO colons. "
                        f"Be specific ‚Äî mention names/products/numbers.\n\n"
                        f"Article: {article_text[:400]}"
                    ),
                },
            ],
            temperature=0.4,
            max_tokens=40,
        )
        raw_title = (title_completion.choices[0].message.content or "").strip()
        import re as _re
        raw_title = raw_title.strip('"\'')
        raw_title = _re.sub(
            r'^(Breaking\s*News|Breaking|BREAKING|Update|Report|News|Spotlight|Alert|'
            r'Headline|Tech|AI|Analysis|Exclusive|Latest|Just\s*In|Flash|Urgent|'
            r'Development|Watch)[:\s‚Äî‚Äì-]+',
            '', raw_title, flags=_re.IGNORECASE
        )
        raw_title = _re.sub(r'^[:\s‚Äî‚Äì-]+', '', raw_title).strip()
        if raw_title and len(raw_title.split()) >= 4:
            title = raw_title
            print(f"üì∞ LLM Title: \"{title}\"")
    except Exception as e:
        print(f"‚ö†Ô∏è LLM title generation failed: {e}")

    # Use article first sentence as fallback title
    if not title:
        fallback = article_text.replace("**", "").replace("- ", "").strip()
        sentences = fallback.split(".")
        title = (sentences[0].strip() + ".") if sentences else "AI Technology News Update"
        title = title[:100]
        print(f"üì∞ Fallback title: \"{title}\"")

    # 7. Generate image prompt with varied styles (title is now available)
    image_prompt = ""

    # Rotate styles for variety
    IMAGE_STYLES = [
        {
            "style": "Cinematic thumbnail",
            "camera": "Shot on RED Komodo, 50mm anamorphic lens, dramatic cinematic lighting",
            "look": "Movie poster composition, rich contrast, deep shadows with highlights, bokeh background",
            "colors": "deep teal and orange color grading, dark moody tones",
        },
        {
            "style": "Realistic photojournalism",
            "camera": "Shot on Canon EOS R5, 35mm lens, natural available light",
            "look": "Reuters/AP wire photo style, documentary feel, real-world environment",
            "colors": "natural muted earth tones, neutral grays and warm browns",
        },
        {
            "style": "Vibrant colorful editorial",
            "camera": "Shot on Sony A7IV, 24-70mm lens, golden hour soft light",
            "look": "Magazine cover quality, vivid saturated colors, clean modern composition",
            "colors": "vibrant blues, warm oranges, rich greens, bold contrasting palette",
        },
        {
            "style": "Dramatic high-contrast",
            "camera": "Shot on Nikon Z9, 85mm lens, single directional hard light",
            "look": "High contrast black and white tones with selective color, powerful silhouettes",
            "colors": "deep blacks, bright whites, one accent color pop (red or blue or gold)",
        },
        {
            "style": "Warm editorial portrait",
            "camera": "Shot on Fujifilm GFX, 110mm lens, soft diffused window light",
            "look": "Intimate close-up perspective, soft focus background, professional warmth",
            "colors": "warm amber, soft cream, honey gold tones, cozy atmosphere",
        },
    ]
    chosen_style = random.choice(IMAGE_STYLES)
    print(f"üé® Image style: {chosen_style['style']}")

    try:
        img_completion = cerebras_client.chat.completions.create(
            model="llama3.1-8b",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are an expert editorial photo director. Write highly detailed image descriptions for news thumbnails. "
                        "Your descriptions MUST match the article topic exactly. "
                        "Include: specific scene/setting, key objects, people or places, background elements, lighting, color palette, mood. "
                        "Output ONLY the description. Never include text, words, logos, watermarks, or UI elements."
                    ),
                },
                {
                    "role": "user",
                    "content": (
                        f"Write an 80-120 word image description that matches this news article.\n\n"
                        f"Title: {title}\n"
                        f"Article: {article_text[:500]}\n\n"
                        f"STYLE: {chosen_style['style']}\n"
                        f"CAMERA: {chosen_style['camera']}\n"
                        f"LOOK: {chosen_style['look']}\n"
                        f"COLORS: {chosen_style['colors']}\n\n"
                        f"Requirements:\n"
                        f"- Describe specific real-world objects, people, or places relevant to the story\n"
                        f"- Include company logos, product designs, or brand elements if relevant\n"
                        f"- Vary the background ‚Äî use offices, labs, streets, factories, nature, conferences\n"
                        f"- NO plain single-color backgrounds, NO simple drawings, NO abstract art\n"
                        f"- Make it look like a professional news thumbnail"
                    ),
                },
            ],
            temperature=0.85,
            max_tokens=200,
        )
        image_prompt = (img_completion.choices[0].message.content or "").strip()
        if image_prompt.startswith('"') and image_prompt.endswith('"'):
            image_prompt = image_prompt[1:-1]
        print(f'üé® Image prompt ({len(image_prompt.split())} words): "{image_prompt[:120]}..."')
    except Exception as e:
        print(f"‚ö†Ô∏è Image prompt generation failed: {e}")
        image_prompt = (
            "Editorial photograph of modern technology workspace, golden-hour light, "
            "minimalist desk with monitors, shallow depth-of-field, Canon EOS R5, "
            "natural color palette, cinematic 16:9"
        )


    # 8. Use AI-picked category (primary), fallback to keyword detection
    if ai_category:
        if ai_category != category:
            print(f"üìå AI category: {ai_category} (keyword was: {category})")
        category = ai_category
    else:
        # Fallback: re-validate with keyword detection
        refined_category = detect_category(search_query, title, article_text)
        if refined_category != category:
            print(f"üìå Category refined: {category} ‚Üí {refined_category} (keyword fallback)")
            category = refined_category

    # 9. Generate image via g4f + upload to Cloudinary
    article_id = str(uuid.uuid4())
    image_url = generate_and_upload_image(image_prompt, article_id)

    # 10. Save to Firestore

    news_item = {
        "id": article_id,
        "title": title,
        "summary": article_text,
        "image_url": image_url,
        "source_link": None,
        "source_name": "XeL AI News",
        "category": category,
        "date": datetime.now(timezone.utc).isoformat(),
    }

    db.collection(COLLECTION).document(article_id).set(news_item)
    save_to_history(db, title, article_text, source_urls)

    duration = int((time.time() - t0) * 1000)
    print(f'‚úÖ Saved: "{title}" in {duration}ms')

    # 10. Log health
    log_health(db, "‚úÖ Success", {
        "last_news_title": title,
        "category": category,
        "word_count": str(word_count),
        "image_prompt": image_prompt[:100],
        "has_image": "yes" if image_url else "no",
        "image_source": "cloudinary" if "cloudinary" in image_url else "placeholder",
        "search_query": used_query,
        "search_results": str(len(scraped_data)),
        "duration_ms": str(duration),
    })

    print(f"\n{'='*60}")
    print(f"‚úÖ Pipeline complete!")
    print(f"   Title:    {title}")
    print(f"   Category: {category}")
    print(f"   Words:    {word_count}")
    print(f"   Image:    {'Cloudinary' if 'cloudinary' in image_url else 'Placeholder'}")
    print(f"   Duration: {duration}ms")
    print(f"{'='*60}")

    return news_item


# ‚îÄ‚îÄ‚îÄ Entry Point ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

if __name__ == "__main__":
    try:
        result = generate_news()
        print(f"\nüìÑ Result: {json.dumps({'title': result['title'], 'category': result['category']}, indent=2)}")
    except Exception as e:
        print(f"\n‚ùå Pipeline failed: {e}")
        # Try to log failure to Firestore
        try:
            db = init_firebase()
            log_health(db, "‚ùå Failed", {"error_message": str(e), "runner": "github-actions"})
        except Exception:
            pass
        sys.exit(1)
